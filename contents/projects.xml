<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Darius A. Görgen</title>
<link>https://dariusgoergen.com/contents/projects.html</link>
<atom:link href="https://dariusgoergen.com/contents/projects.xml" rel="self" type="application/rss+xml"/>
<description>Geoinformation and Environmental Sciences</description>
<generator>quarto-1.1.149</generator>
<lastBuildDate>Fri, 17 Apr 2020 11:47:00 GMT</lastBuildDate>
<item>
  <title>Tree species classification based on UAV orthoimages</title>
  <dc:creator>Darius A. Görgen</dc:creator>
  <link>https://dariusgoergen.com/contents/projects/2020-04-17-pheno/index.html</link>
  <description><![CDATA[ 



<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://dariusgoergen.com/contents/projects/2020-04-17-pheno/drone.jpg" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Image of a flying drone.</figcaption><p></p>
</figure>
</div>
<section id="what-is-it-about" class="level3">
<h3 class="anchored" data-anchor-id="what-is-it-about">What is it about?</h3>
<p>Supervised machine learning algorithms can help to extract useful information from high-dimensional datasets to benefit environmental conversation efforts. The correct identification of tree species based on imagery collected by low-cost UAVs and conventional cameras is undoubtedly a promising advancement in technology which might reduce the cost of local forest monitoring. Because the broader use of this technology only occurred very recently, structural investigations into the benefits and the limitations of this approach are limited. As a student’s team, we set out to investigate the relationship between the classification accuracy and different spatial resolutions of the imagery. Additionally, we were interested in the question of whether or not predictor variables calculated based on multi-temporal observations throughout the growing season enhances the classification.</p>
</section>
<section id="what-can-it-do" class="level3">
<h3 class="anchored" data-anchor-id="what-can-it-do">What can it do?</h3>
<p>We established an empirical experiment to investigate the influence of spatial resolution and mono- vs.&nbsp;multi-temporal predictor variables using the Random Forest algorithm. We used 5-fold cross-validation combined with the Leave-Location-Out approach (LLOCV) to train a total number of 9 models. These included each combination of three different spatial resolutions (10, 15, and 25 cm) and three different combinations of the predictor variables (mono-temporal, seasonal, and both predictor sets.)</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://dariusgoergen.com/contents/projects/2020-04-17-pheno/acc.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Accuracies of a 5-fold cross-validation</figcaption><p></p>
</figure>
</div>
<p>We learned that a medium resolutions seems beneficial and that seasonal parameters are able to increase the classification accuracy at about 1-2 %. There were also indications, that object-based classification has the potential to significantly increase the overall accuracy.</p>
<hr>
<p>Interested in the results? Check out our written <a href="https://github.com/goergen95/forestPhenology/blob/master/doc/report.pdf">report</a> or browse through our <a href="https://github.com/goergen95/forestPhenology/">R code workflow</a>.</p>


</section>

 ]]></description>
  <category>UAV</category>
  <category>RandomForest</category>
  <category>R</category>
  <guid>https://dariusgoergen.com/contents/projects/2020-04-17-pheno/index.html</guid>
  <pubDate>Fri, 17 Apr 2020 11:47:00 GMT</pubDate>
  <media:content url="https://dariusgoergen.com/contents/projects/2020-04-17-pheno/drone.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Aerosol-Cloud Interactions and Precipitation in the Aral Sea basin</title>
  <dc:creator>Darius A. Görgen</dc:creator>
  <link>https://dariusgoergen.com/contents/projects/2020-03-20-aciASB/index.html</link>
  <description><![CDATA[ 



<section id="what-is-it-about" class="level3">
<h3 class="anchored" data-anchor-id="what-is-it-about">What is it about?</h3>
<p>Once the largest terminal lake in Central Asia, the Aral Sea is fed by its tributaries, the Syr Darya and the Amu Darya. It has suffered from a significant reduction of water inflow during the last two decades. Consequently, the water retreated from large portions of the former sea’s surface, forming a dessert called Aralkum. The diffusion of dust and sand particles in the area may threaten human health and agricultural productivity. Also, concerns about impacts to the regional hydrological cycle through the increase of aerosols in the atmosphere have been raised.</p>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://dariusgoergen.com/contents/projects/2020-03-20-aciASB/animation.gif" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Shrinking of the Aral Sea (Source: NASA 2020).</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>(Source: <a href="https://earthobservatory.nasa.gov/world-of-change/AralSea">NASA 2019</a>)</p>
<p>The interaction of aerosols with cloud micro-physics is very complex and one of the mechanisms in our atmosphere, we barely understand (<a href="https://www.ipcc.ch/site/assets/uploads/2018/02/WG1AR5_Chapter07_FINAL-1.pdf">Boucher et al.&nbsp;2013</a>). Besides, our understanding of the process is subject to inaccuracies due to limited observational capacity. For example, from passive optical instruments aboard satellites, it is only possible to retrieve information on aerosols when clear-sky conditions are met. Evidently, the opposite is valid for cloud parameters, rendering every analysis to significant differences at the time scale between the measured properties of clouds and aerosols. Additionally, the parameters which are derived are generalized for the vertical atmospheric column for every spatial unit, meaning we do not get any information about the vertical distribution of neither aerosols nor clouds.</p>
</section>
<section id="what-it-can-do" class="level3">
<h3 class="anchored" data-anchor-id="what-it-can-do">What it can do!</h3>
<p>The mechanisms which govern the cloud-aerosol interactions mainly depend on the type of aerosol, whether it acts hydrophilic or hydrophobic, and its size. Some mechanisms are expected to suppress precipitation, while others are suspected of leading to more severe and intense precipitation events. To investigate the relationship between aerosols and precipitation, I made use of the MODIS <a href="https://modis.gsfc.nasa.gov/data/dataprod/mod06.php">cloud</a> and <a href="https://modis.gsfc.nasa.gov/data/dataprod/mod04.php">aerosol</a> products as well as the <a href="https://www.chc.ucsb.edu/data/chirps">CHIRPS</a> data set.<br>
The former uses satellite and ground observations to retrieve rainfall rates at a monthly resolution. The cloud and aerosol parameters were aggregated to the same spatial and temporal resolution to allow a correlation analysis.</p>
<hr>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://dariusgoergen.com/contents/projects/2020-03-20-aciASB/cor_AOD_550_RH.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Correlation AOD and P</figcaption><p></p>
</figure>
</div>
<p>During spring (a) and winter (d), a negative relationship between the Aerosol Optical Depth (AOD) and precipitation rates (P) govern the study area. During the other two seasons, we also see pixels with positive correlations. However, these are not significant (pixels with a significant correlation are marked by crosses). The analysis was done in R and included data from 2003 to 2018 because since then, both MODIS satellites delivered observations for the study area. The source code of the complete project can be found in a <a href="https://github.com/goergen95/aciASB">GitHub repository</a>, along with a more comprehensive <a href="https://github.com/goergen95/aciASB/blob/master/docs/report_II.pdf">discussion</a> of the results.</p>


</section>

 ]]></description>
  <category>MODIS</category>
  <category>Clouds</category>
  <category>Precipitaion</category>
  <category>R</category>
  <guid>https://dariusgoergen.com/contents/projects/2020-03-20-aciASB/index.html</guid>
  <pubDate>Mon, 20 Jan 2020 15:06:00 GMT</pubDate>
  <media:content url="https://dariusgoergen.com/contents/projects/2020-03-20-aciASB/cor_AOD_550_RH.png" medium="image" type="image/png" height="104" width="144"/>
</item>
<item>
  <title>Machine Learning to classify microplastic particles</title>
  <dc:creator>Darius A. Görgen</dc:creator>
  <link>https://dariusgoergen.com/contents/projects/2019-10-15-polymeRID/index.html</link>
  <description><![CDATA[ 



<p><img src="https://dariusgoergen.com/contents/projects/2019-10-15-polymeRID/seperators.jpg" class="img-fluid" alt="Probe Seperators"><br> <em>Photo of two sediment separators taken by Sarah Brüning</em></p>
<section id="what-is-it-about" class="level3">
<h3 class="anchored" data-anchor-id="what-is-it-about">What is it about?</h3>
<p>Microplastic particles in the environment are an ever-growing concern in public and science. It is suspected not only to pose threats to wildlife but also concerns for human health have been raised. Scientists and especially decision-makers depend on the availability of precise and timely information on the where and when’s of microplastic in the environment. However, the process of spectral identification of particles remains a cost-intensive process requiring humans to invest time and effort into the correct identification of spectra. Modern machine learning techniques have the potential to decrease the necessity of human intervention in the classification process and thus significantly speed up the complete process from taking environmental samples to communicate the results to the public.</p>
</section>
<section id="what-it-can-do" class="level3">
<h3 class="anchored" data-anchor-id="what-it-can-do">What it can do!</h3>
<p>Based on a freely available reference data base published by <a href="https://link.springer.com/article/10.1007/s00216-018-1156-x">Primpke and colleagues</a> I developed a decision fusion algorithm based on two Random Forest models and two Convolutional Neural Networks which together can robustly identify polymers based on FTIR-spectral data. The complete workflow is reproducible and adjustable and consists of the following stages: preparation of a reference data base, exploration of machine learning models and pre-processing techniques, calibration of the final decision fusion algorithm, and classification of real-world spectral samples. The algorithm was written in R and the code is available on a <a href="https://github.com/goergen95/polymeRID">GitHub repository</a>. Additionally, there is a <a href="https://goergen95.github.io/polymeRID/index.html">workflowr website</a> communicating the structure of the algorithm and the results based on the mentioned OpenSource data base.</p>
<hr>
<p>Here you can explore the spectral reference data base based on selected polymers and other materials commonly found in environmental samples. Simply choose a class you wish to display and explore its spectral characteristics.</p>
<div class="cell">
<iframe src="https://goergen95.shinyapps.io/shiny_apps/?showcase=0" width="672" height="500px" data-external="1">
</iframe>
</div>


</section>

 ]]></description>
  <category>Machine Learning</category>
  <category>Microplastics</category>
  <category>CNN</category>
  <category>FTIR-Spectrometry</category>
  <category>R</category>
  <guid>https://dariusgoergen.com/contents/projects/2019-10-15-polymeRID/index.html</guid>
  <pubDate>Tue, 22 Oct 2019 13:43:00 GMT</pubDate>
  <media:content url="https://dariusgoergen.com/contents/projects/2019-10-15-polymeRID/seperators.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Low-cost SensorBoxes for automated environmental monitoring</title>
  <dc:creator>Darius A. Görgen</dc:creator>
  <link>https://dariusgoergen.com/contents/projects/2019-07-01-sensorboxes/index.html</link>
  <description><![CDATA[ 



<p><img src="https://dariusgoergen.com/contents/projects/2019-07-01-sensorboxes/sensorbox.png" class="img-fluid" alt="Screenshot"> <em>Screenshot of the project website.</em></p>
<section id="what-is-it-about" class="level3">
<h3 class="anchored" data-anchor-id="what-is-it-about">What is it about?</h3>
<p>This project website documents a master’s seminar work at the University of Marburg on a fully automated, low-cost, and self-build environmental sensor unit used in the LOEWE research project <a href="https://www.uni-marburg.de/de/fb19/natur40">Natur4.0</a>. Its core part is a Raspberry Pi 3 (Model B) equipped with sensors to measure incoming radiation, temperature, and humidity, as well as a camera and a microphone to take records of animals in forest environments.</p>
<hr>
<p>Core Components</p>
<ul>
<li>Raspberry Pi 3 (B)</li>
<li>Raspberry Pi Camera Module v2.1</li>
<li>SF-555 Microphone (Foxnovo)</li>
<li>DHT22-AM2302 Temperature and Humidity Sensor</li>
<li>TSL2591 High Dynamic Range Digital Light Sensor</li>
<li>KY-024 Hall Sensors</li>
<li>DS3231 Real-Time Clock</li>
<li>PowerBank and Wireless Charging Pads</li>
</ul>
<hr>
</section>
<section id="what-it-can-do" class="level3">
<h3 class="anchored" data-anchor-id="what-it-can-do">What it can do!</h3>
<p>The SensorBox is a fully automated sensing unit that can be used in conjunction with a cable car mounted on specific trees to monitor environmental variables on a vertical gradient within forest structures. There is a need to regularly charge the batteries on the ground station and in case of the presence of a WLAN infrastructure, the SensorBox can send the collected data to another station. It can help to collect data on valuable parameters which can be used together with remotely sensed imagery, e.g., by UAVs, to predict the spatial distribution of these parameters below the canopy cover.</p>
<hr>
<p><a href="https://goergen95.github.io/sensorbox-docu">Check it out</a> here or follow the link to <a href="https://github.com/goergen95/sensorbox-docu">GitHub</a> to explore the source code of the website. Note that the content shown here comes from a student project and only covers the status during our two-week seminar. Check out the <a href="https://nature40.github.io/Nature40DocumentationProject/">official documentation</a> for more recent information.</p>


</section>

 ]]></description>
  <category>Environment</category>
  <category>Monitoring</category>
  <category>Raspberry Pi</category>
  <guid>https://dariusgoergen.com/contents/projects/2019-07-01-sensorboxes/index.html</guid>
  <pubDate>Sat, 20 Jul 2019 22:10:00 GMT</pubDate>
  <media:content url="https://dariusgoergen.com/contents/projects/2019-07-01-sensorboxes/sensorbox.png" medium="image" type="image/png" height="83" width="144"/>
</item>
<item>
  <title>Forest stand analysis based on remote sensing</title>
  <dc:creator>Darius A. Görgen</dc:creator>
  <link>https://dariusgoergen.com/contents/projects/2019-05-31-mof/index.html</link>
  <description><![CDATA[ 



<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://dariusgoergen.com/contents/projects/2019-05-31-mof/biodiv.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Biodiversity Map</figcaption><p></p>
</figure>
</div>
<p><em>Tree species biodiversity map.</em></p>
<section id="what-is-it-about" class="level3">
<h3 class="anchored" data-anchor-id="what-is-it-about">What is it about?</h3>
<p>Forests represent the most diverse habitat for different species around the globe. Their monitoring is one of the most crucial tasks for biodiversity management. Traditional means of monitoring forests are cost and labor-intensive, which leads to low revisit frequencies and small monitored areas. Additionally, the results of data acquisitions by human agents make the results hardly reproducible. To overcome these limitations, a LOEWE research project called <a href="https://www.uni-marburg.de/de/fb19/natur40">Natur4.0</a> has been initiated between several German research institutes. In this project, I participated in a student’s seminar and analyzed tree species and forest structures employing remote sensing techniques.</p>
</section>
<section id="what-it-can-do" class="level3">
<h3 class="anchored" data-anchor-id="what-it-can-do">What it can do!</h3>
<p>In this project, I used RGB orthoimages and a point cloud derived from Light Detection and Ranging (LiDAR) data to train a segmentation algorithm to distinguish between individual trees based on a Canopy Height Model. This technique is based on a watershed algorithm that “grows” continues segments around a tree’s central position to delineate the total tree crown.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://dariusgoergen.com/contents/projects/2019-05-31-mof/seg_area.gif" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Animation of the tree segmentation</figcaption><p></p>
</figure>
</div>
<p>Once we successfully generated the tree objects, we used an object-based classification of the tree species. As predictors, we used several artificially created indices and filters from RGB images. Finally, we trained a Random Forest model to predict the tree species. Using the point cloud, structural forest parameters, such as vegetation density, can be aggregated on the level of individual trees and then analyzed. We see this result in the picture above where I calculated the <a href="https://www.rdocumentation.org/packages/vegan/versions/2.4-2/topics/diversity"><em>Shannon-Index</em></a> based on the number of different tree species found in a circular 10 meters environment. Green colors show lower numbers of tree species, while red colors indicate a relative species richness.</p>
<hr>
<p>You can check out the results on a comprehensive <a href="https://goergen95.github.io/mof_caldern/index.html">website</a>! You are also invited to read through the <a href="https://github.com/goergen95/mof_caldern">code</a> for the analysis.</p>


</section>

 ]]></description>
  <category>Machine Learning</category>
  <category>LIDAR</category>
  <category>R</category>
  <guid>https://dariusgoergen.com/contents/projects/2019-05-31-mof/index.html</guid>
  <pubDate>Fri, 31 May 2019 14:27:00 GMT</pubDate>
  <media:content url="https://dariusgoergen.com/contents/projects/2019-05-31-mof/biodiv.png" medium="image" type="image/png" height="167" width="144"/>
</item>
</channel>
</rss>

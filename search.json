[
  {
    "objectID": "contents/blog.html",
    "href": "contents/blog.html",
    "title": "Darius A. Görgen",
    "section": "",
    "text": "Migrating to Quarto\n\n\n\n\n\n\n\nR\n\n\nQuarto\n\n\n\n\nSome thoughts on the process of migrating this website to Quarto\n\n\n\n\n\n\nAug 30, 2022\n\n\nDarius A. Görgen\n\n\n\n\n\n\n  \n\n\n\n\nR API to download FAO’s WaPOR datasets\n\n\n\n\n\n\n\nR\n\n\nraster\n\n\nFAO WAPOR\n\n\nAPI\n\n\n\n\nA R API package to query and download FAO’s WaPOR raster datasets.\n\n\n\n\n\n\nOct 31, 2020\n\n\nDarius A. Görgen\n\n\n\n\n\n\n  \n\n\n\n\nRunning docker-compose with postgis and R-Studio\n\n\n\n\n\n\n\nR\n\n\ndocker\n\n\nPostgreSQL\n\n\n\n\nUsing docker-compose to set up a R-Studio container and postgis.\n\n\n\n\n\n\nJun 21, 2020\n\n\nDarius A. Görgen\n\n\n\n\n\n\n  \n\n\n\n\nTranslating EUMETSAT’s .nat files to GTiff\n\n\n\n\n\n\n\nMSG\n\n\nGTIFF\n\n\nPython\n\n\n\n\nPython code to translate EUMETSAT’s .nat datasets to GTiffs.\n\n\n\n\n\n\nMay 10, 2020\n\n\nDarius A. Görgen\n\n\n\n\n\n\n  \n\n\n\n\nTranslating EUMETSAT’s .bfr files to GTiff\n\n\n\n\n\n\n\nBUFR\n\n\nTIFF\n\n\nPython\n\n\n\n\nPython code to translate EUMETSAT’s BUFR datasets to GTiffs.\n\n\n\n\n\n\nMay 1, 2020\n\n\nDarius A. Görgen\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "contents/blog/2020-05-10-bufr2tif/index.html",
    "href": "contents/blog/2020-05-10-bufr2tif/index.html",
    "title": "Translating EUMETSAT’s .bfr files to GTiff",
    "section": "",
    "text": "import numpy as np\nfrom osgeo import gdal\nfrom osgeo import osr\nimport pyresample as pr\nfrom pybufr_ecmwf.bufr import BUFRReader\nimport matplotlib.pyplot as plt\n\nThe BUFR format is a standardized format defined by the World Meteorological Organization (WMO). It stands for Binary Universal Form for the Representation of meteorological data. It is a self-describing format, shipping data together with metadata to be used by end-users. Within a .bfr file, we find several messages, each of them having a specific number of entries. We will use the functionality of the pybufr_ecmwf library to read in the data.\n\nfile = \"MSG2-SEVI-MSGRIIE-0101-0101-20160526000000.000000000Z-20160526000602-1403456.bfr\"\n# read the file\nbufr = BUFRReader(file, warn_about_bufr_size = False, expand_flags = False)\n# display number of messages\nprint(\"Number of messages: \"+ str(bufr.num_msgs))\n\n# initiate list with parameter names\nnames_units = []\nfor m, msg in enumerate(bufr):\n names_units.append(msg.get_names_and_units())\n\n# show parameter names and units\nprint('\\n'.join(map(str, names_units[0][0])))\n# close file\nbufr.close()\n\nNumber of messages: 90\n\n\nSATELLITE IDENTIFIER\nIDENTIFICATION OF ORIGINATING/GENERATING CENTRE (SEE NOTE 10)\nSATELLITE CLASSIFICATION\nSEGMENT SIZE AT NADIR IN X DIRECTION\nSEGMENT SIZE AT NADIR IN Y DIRECTION\nYEAR\nMONTH\nDAY\nHOUR\nMINUTE\nSECOND\nROW NUMBER\nCOLUMN NUMBER\nLATITUDE (HIGH ACCURACY)\nLONGITUDE (HIGH ACCURACY)\nSATELLITE ZENITH ANGLE\nK INDEX\nKO INDEX\nPARCEL LIFTED INDEX (TO 500 HPA)\nMAXIMUM BUOYANCY\nPRECIPITABLE WATER\nPER CENT CONFIDENCE\nPRESSURE\nPRESSURE\nPRECIPITABLE WATER\nPRESSURE\nPRESSURE\nPRECIPITABLE WATER\nPRESSURE\nPRESSURE\nPRECIPITABLE WATER\n\n\nBased on the above output, we can decide which parameters we are interested in and which metadata we will need. Say we are only interested in the parameter K Index. We can see that the index for this dataset is 16. Also, since we are interested in writing a .tiff as output, the datasets of latitude and longitude will be of interest to us (index 13 and 14, respectively). Note that we are reopening the file once again to start from the very first message.\n\n# initiate arrays\nlats = np.zeros([0])\nlons = np.zeros([0])\nvals = np.zeros([0])\n# reopening the file\nbufr = BUFRReader(file, warn_about_bufr_size = False, expand_flags = False)\n\n# loop through the messages and sub-entries\nfor msg in bufr:\n for subs in msg:\n  data = subs.data\n  lats = np.append(lats, data[:,13])\n  lons = np.append(lons, data[:,14])\n  vals = np.append(vals, data[:,20])\n# don't forget to close the file\nbufr.close()\nvals = np.where(vals == np.max(vals), -9999, vals)\n\nWith this loop, we obtained all the necessary data to create a .tiff file. We have got the values we are interested in and the geographic information of each location’s latitude and longitude data. We can now use the pyresample library to resample our data to a location of interest. Let’s say we are interested in a study area roughly having the extent of France. We can resample to this area by declaring an area definition first.\n\n# define some general properties of our projection\narea_id = \"France\"\ndescription = \"Custom Geographical CRS of France\"\nproj_id = \"France WGS84 geographical\"\nproj_dict = {\"proj\": \"longlat\", \"ellps\":\"WGS84\", \"datum\": \"WGS84\"}\n# define the area's extent in degrees and desired resolution\nllx = -4.9\nlly = 42.2\nurx = 8.2\nury = 51.2\nres = 0.015 # in degrees\nwidth = int((urx - llx) / res)\nheight = int((ury - lly) / res)\narea_extent = (llx,lly,urx,ury)\narea_def = pr.geometry.AreaDefinition(area_id, proj_id, description, proj_dict, width, height, area_extent)\nprint(area_def)\n\nArea ID: France\nDescription: France WGS84 geographical\nProjection ID: Custom Geographical CRS of France\nProjection: {'datum': 'WGS84', 'no_defs': 'None', 'proj': 'longlat', 'type': 'crs'}\nNumber of columns: 873\nNumber of rows: 600\nArea extent: (-4.9, 42.2, 8.2, 51.2)\n\n\n/home/darius/Desktop/website/new/py-env/lib/python3.10/site-packages/pyproj/crs/crs.py:1282: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems\n  proj = self._crs.to_proj4(version=version)\n\n\nWith this area definition, we can resample our data using the nearest neighbor algorithm and use our defined variables about the location to create a .tiff file as output.\n\nswath_def = pr.geometry.SwathDefinition(lons = lons, lats = lats)\nres_data = pr.kd_tree.resample_nearest(swath_def, vals, area_def,\nradius_of_influence=16000,epsilon=0.5,fill_value=False)\n\n# create tif output\nfilename = \"bufr2tif.tif\"\n# number of rows and cols\nrows = res_data.shape[0]\ncols = res_data.shape[1]\n# pixel size\npixelWidth = (area_def.area_extent[2] - area_def.area_extent[0]) / cols\npixelHeight = (area_def.area_extent[1] - area_def.area_extent[3]) / rows\n# pixel of origin\noriginX = area_def.area_extent[0]\noriginY = area_def.area_extent[3]\n# driver\ndriver = gdal.GetDriverByName(\"GTiff\")\n# create file \noutRaster = driver.Create(filename, cols, rows, 1, gdal.GDT_Float32)\n# set resoultion and origin\noutRaster.SetGeoTransform((originX, pixelWidth, 0, originY, 0, pixelHeight))\n# create one band\noutband = outRaster.GetRasterBand(1)\n# Float_64 no data value (or customize)\noutband.SetNoDataValue(-9999)\n# write the resampled data to file\noutband.WriteArray(np.array(res_data))\n# create a spatial reference system\noutRasterSRS = osr.SpatialReference()\noutRasterSRS.ImportFromEPSG(4326)\n# write SRS to file\noutRaster.SetProjection(outRasterSRS.ExportToWkt())\n# clean up\noutband.FlushCache()\noutband = None\noutRaster = None\n\n\nNow we can read in the newly created file and look at a simple plot to visualize our result. Note that, in the background, I am using R to generate this plot quickly.\n\nfile = \"bufr2tif.tif\"\nds = gdal.Open(file)\nband = ds.GetRasterBand(1)\ndata = band.ReadAsArray()\ndata = np.where(data == -9999., np.NaN, data)\nplt.imshow(data)\n\n<matplotlib.image.AxesImage at 0x7f4afc87fb80>"
  },
  {
    "objectID": "contents/blog/2020-06-14-nat2tif/index.html",
    "href": "contents/blog/2020-06-14-nat2tif/index.html",
    "title": "Translating EUMETSAT’s .nat files to GTiff",
    "section": "",
    "text": "import numpy as np\nfrom osgeo import gdal\nfrom osgeo import osr\nimport os\nimport pyresample as pr\nfrom satpy import Scene\nimport matplotlib.pyplot as plt\n\nAs opposed to BUFR files, reading .nat files is quite straightforward. All we need to do is handing the right reader to the satpy Scene function. We can then take a look at the available datasets.\n\nfile = \"MSG1-SEVI-MSG15-0100-NA-20160531090417.660000000Z-20160531090437-1405098.nat\"\n# define reader\nreader = \"seviri_l1b_native\"\n# read the file\nscn = Scene(filenames = {reader:[file]})\n# extract data set names\ndataset_names = scn.all_dataset_names()\n# print available datasets\nprint('\\n'.join(map(str, dataset_names)))\n\nHRV\nIR_016\nIR_039\nIR_087\nIR_097\nIR_108\nIR_120\nIR_134\nVIS006\nVIS008\nWV_062\nWV_073\n\n\nThe MSG data is provided as Full Disk, meaning that roughly the complete North-South extent of the globe from the Atlantic to the Indian Ocean is present in each file. For most applications and research questions, it is not necessary to process an extent that large. This is why as an example, we are going to resample the data to the extent of Spain. For this, we are using functionality from the pyresample library, which allows users to create customized area definitions.\n\n# create some information on the reference system\narea_id = \"Spain\"\ndescription = \"Geographical Coordinate System clipped on Spain\"\nproj_id = \"Spain\"\n# specifing some parameters of the projection\nproj_dict = {\"proj\": \"longlat\", \"ellps\": \"WGS84\", \"datum\": \"WGS84\"}\n# calculate the width and height of the aoi in pixels\nllx = -9.5 # lower left x coordinate in degrees\nlly = 35.9 # lower left y coordinate in degrees\nurx = 3.3 # upper right x coordinate in degrees\nury = 43.8 # upper right y coordinate in degrees\nresolution = 0.005 # target resolution in degrees\n# calculating the number of pixels\nwidth = int((urx - llx) / resolution)\nheight = int((ury - lly) / resolution)\narea_extent = (llx,lly,urx,ury)\n# defining the area\narea_def = pr.geometry.AreaDefinition(area_id, proj_id, description, proj_dict, width, height, area_extent)\nprint(area_def)\n\nArea ID: Spain\nDescription: Spain\nProjection ID: Geographical Coordinate System clipped on Spain\nProjection: {'datum': 'WGS84', 'no_defs': 'None', 'proj': 'longlat', 'type': 'crs'}\nNumber of columns: 2560\nNumber of rows: 1579\nArea extent: (-9.5, 35.9, 3.3, 43.8)\n\n\n/home/darius/Desktop/website/new/py-env/lib/python3.10/site-packages/pyproj/crs/crs.py:1282: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems\n  proj = self._crs.to_proj4(version=version)\n\n\nWe will show here how to proceed when we want to extract more than one specific data set. We can either apply a for loop over the desired datasets we need or write a general function that can extract the data for any specified variable. Here we are going forward with the latter approach because a function is more reusable than a simple script.\n\ndef nat2tif(file, calibration, area_def, dataset, reader, outdir, label, dtype, radius, epsilon, nodata):\n  # open the file\n  scn = Scene(filenames = {reader: [file]})\n  # let us check that the specified data set is actually available\n  scn_names = scn.all_dataset_names()\n  # raise exception if dataset is not present in available names\n  if dataset not in scn_names:\n    raise Exception(\"Specified dataset is not available.\")\n  # we need to load the data, different calibration can be chosen\n  scn.load([dataset], calibration=calibration)\n  # let us extract the longitude and latitude data\n  lons, lats = scn[dataset].area.get_lonlats()\n  # now we can apply a swath definition for our output raster\n  swath_def = pr.geometry.SwathDefinition(lons=lons, lats=lats)\n  # and finally we also extract the data\n  values = scn[dataset].values\n  # we will now change the datatype of the arrays\n  # depending on the present data this can be changed\n  lons = lons.astype(dtype)\n  lats = lats.astype(dtype)\n  values = values.astype(dtype)\n  # now we can already resample our data to the area of interest\n  values = pr.kd_tree.resample_nearest(swath_def, values,\n                                             area_def,\n                                             radius_of_influence=radius, # in meters\n                                             epsilon=epsilon,\n                                             fill_value=False)\n  # we are going to check if the outdir exists and create it if it doesnt\n  if not os.path.exists(outdir):\n   os.makedirs(outdir)\n  # let us join our filename based on the input file's basename                                           \n  outname = os.path.join(outdir, os.path.basename(file)[:-4] + \"_\" + str(label) + \".tif\")\n  # now we define some metadata for our raster file\n  cols = values.shape[1]\n  rows = values.shape[0]\n  pixelWidth = (area_def.area_extent[2] - area_def.area_extent[0]) / cols\n  pixelHeight = (area_def.area_extent[1] - area_def.area_extent[3]) / rows\n  originX = area_def.area_extent[0]\n  originY = area_def.area_extent[3] \n  # here we actually create the file\n  driver = gdal.GetDriverByName(\"GTiff\")\n  outRaster = driver.Create(outname, cols, rows, 1)\n  # writing the metadata\n  outRaster.SetGeoTransform((originX, pixelWidth, 0, originY, 0, pixelHeight))\n  # creating a new band and writting the data\n  outband = outRaster.GetRasterBand(1)\n  outband.SetNoDataValue(nodata) #specified no data value by user\n  outband.WriteArray(np.array(values)) # writting the values\n  outRasterSRS = osr.SpatialReference() # create CRS instance\n  outRasterSRS.ImportFromEPSG(4326) # get info for EPSG 4326\n  outRaster.SetProjection(outRasterSRS.ExportToWkt()) # set CRS as WKT\n  # clean up\n  outband.FlushCache()\n  outband = None\n  outRaster = None\n\nNow we can apply this function to our input file and extract any available dataset. Note that some of the input variables need further explanation. The very first option which might not be self-evident is calibration. With this option we can tell satpy to pre-calibrate the data, for example, to reflectance in contrast to radiances. The option label appends the value of label to the ouput filename. With the dtype option, we can specifically choose which datatype is used for the output file. Accordingly, we should adopt the value for nodata, which flags no data values in the output file. The options radius and epsilon are options of the nearest neighbor resampling routine and can be specified to the user needs (see here for more information).\n\nnat2tif(file = file, \n        calibration = \"radiance\",  \n        area_def = area_def,  \n        dataset = \"HRV\", \n        reader = reader, \n        outdir = \"./output\",  \n        label = \"HRV\", \n        dtype = \"float32\", \n        radius = 16000, \n        epsilon = .5, \n        nodata = -3.4E+38)\n\nNow we can read in the newly created file and take a look at a simple plot to visualize our result (Note that in the background, I am using R to generate this plot quickly).\n\nfile = \"output/MSG1-SEVI-MSG15-0100-NA-20160531090417.660000000Z-20160531090437-1405098_HRV.tif\"\nds = gdal.Open(file)\nband = ds.GetRasterBand(1)\ndata = band.ReadAsArray()\nplt.imshow(data)\n\n<matplotlib.image.AxesImage at 0x7f54fa1e1c90>"
  },
  {
    "objectID": "contents/blog/2020-06-21-docker.md/index.html",
    "href": "contents/blog/2020-06-21-docker.md/index.html",
    "title": "Running docker-compose with postgis and R-Studio",
    "section": "",
    "text": "What we need is a simple .yaml file that defines the parameters of our application. In the example I will demonstrate, the application consists of two services, namely the postigs server and the R-Studio instance. We start with the postgres service by defining which images we are using. This image is found at dockerhub If the images are not found on the local machine, they are pulled from there once we run our application. We also make the container’s naming explicit by using the container_name option and set the restart behavior to always in cases the container breaks down. We also ensure that the container will set up a new user besides the default postgres user by specifying a username and a password. The last part specifies on the left hand the local path on the host machine where the database files are going to be written two, while the right-hand part links to the data directory within the postgis container.\nservices:\n  postgres:\n  image: postgis/postgis\n  container_name: postgres\n  restart: always\n  environment:\n    POSTGRES_USER: postgis\n    POSTGRES_PASSWORD: postgis\n  ports:\n    - 8888:5432\n  volumes:\n    - ~/pgdata:/var/lib/postgresql/data\n\nrstudio:\n  image: rocker/geospatial\n  container_name: r-studio\n  restart: always\n  environment:\n    - USER=rstudio\n    - PASSWORD=supersecret\n    - ROOT=TRUE\n  ports:\n    - 8787:8787\n  links: \n    - postgres\nFor the R-Studio service, we use the geospatial image that comes in very handy in cases you want to do geospatial analysis since a large number of important packages already come preinstalled. We also make the naming of the container explicit and set the restart behavior to always. The rocker images are quite restrictive when it comes to user rights management. Thus, in addition to specifying a user and a password, we also want to enable root access for the case we need to install additional software. This way, our user is added to the sudoer list. However, this option should be treated carefully. We also map a port on the host machine to the exposed port of the container. This way, we can reach the interface by simply accessing http://localhost:8787/ in the browser of our choice. Finally, we declare that the posgres service is linked to the rstudio service. This means that inside the rstudio container, an entry to /etc/hosts called postgres is added, linking to the IP-address this container is found.\nWhen we write the above configuration to a file called docker-compose.yaml starting the application is as simple as running the following command in a shell in the same directory the file is found:\ndocker-compose up\nOnce the services are up and running, visit http://localhost:8787/ and enter the username and password for the rstudio service. From here, we can use the RPostgreSQL and RPostgres library to connect to our data base. In the simple example below, we establish a connection and write an sf-Object shipped with the sf package to the database.\nlibrary(RPostgreSQL)\nlibrary(RPostgres)\nlibrary(sf)\n\n# specify connection inputs\ndb <- 'postgis'  # provide the name of your db\nhost_db = \"postgres\" # provide the name of the service\ndb_port = '5432'  # specify the port the service is available at\ndb_user = \"postgis\"  # specify username\ndb_password = \"postgis\" # specify password\n# establish the connection\ncon <- dbConnect(Postgres(), \n                 dbname=db, \n                 host=host_db, \n                 port=db_port, \n                 user=db_user, \n                 password=db_password)\n\n# read sample shapefile\nshape = st_read(system.file(\"gpkg/nc.gpkg\", package = \"sf\"))\n# write to database in a new table specified by the layer argument\ntable_name = \"test\"\nst_write(obj = shape, dsn = con, layer = table_name)\nSince we mapped the data directory to a local directory on our host machine, the data we write into the database is persistent even when the application is no longer running. To query our database now or even after restarting the application we can use more sf functionality to interact with it. Here is a simple example comparing dplyr syntax with the result we obtain from querying the database to unify all polygons which have the same entry in the ‘SID74’ variable.\nlibrary(dplyr)\n\n# dplyr way of \nshape_dplyr = shape %>%\n  group_by(SID74) %>%\n  summarise(AREA = sum(AREA, na.rm = TRUE))\n\nshape_postgis = st_read(con, query = sprintf(paste0('SELECT \"SID74\", SUM(\"AREA\") as \"AREA\", ST_UNION(geom) as geom \\n',\n                                    'FROM \\\"%s\\\" \\n',\n                                    'GROUP BY \"SID74\";')), table_name)\nBut how can we manage our postgis server now? There are two ways (well, actually three ways I’ll explain in a minute) how we can access the server. The first one is using psql from the host machine. In the configuration file, we specified that the postgis containers port is mapped to the host machine’s port 8888. Well, we simply use that information to connect to the database using psql.\npsql --host localhost --port 8888 --dbname postgres --user postgis\nThe command line will ask us for the password, and then we are connected. Additionally, we can also choose to use a program with a graphical user interface to manage the data base. One which ich highly recommend is pgAdmin. After installing and running the program, we have to specify a new connection. After adding a name for the connection in the General tab we switch to the Connection tab and fill in all the information as in the screenshot below.\n\n\n\npgAdmin settings tab\n\n\nYou already guessed the third option I promised you before? Well of course we can use the psql approach explained above from the terminal in our R-Studio container. Of course, the hostname will be slightly different, but the basic idea is the same. Now, it comes in handy that we have root access in the R-Studio container, because we are going to need to install external software first, namely the postgres client. Thus we enter the following two lines into the terminal. Enter Yes in case you are asked if you wanted to install the software.\nsudo apt update\nsudo apt install postgresql-client\nNow our container has the psql command installed and we can use a slightly different version of the above command to connect to the databse.\npsql --host postgres --port 5432 --dbname postgres --user postgis\nWhat are the differences here? Well, first the container is not found at localhost but as explained before we have got an entry in the /etc/hosts file for our container name postgres. Additionally, we are connecting directly to the container and not to the mapped port on our host machine which is why we have to specify 5432 as the right port. Besides that, everything remains the same."
  },
  {
    "objectID": "contents/blog/2020-10-31-wapoR/index.html",
    "href": "contents/blog/2020-10-31-wapoR/index.html",
    "title": "R API to download FAO’s WaPOR datasets",
    "section": "",
    "text": "Screenshot of from the WaPOR website\n\nWhat is it about?\nThe WaPOR project by FAO offers some awesome remote sensing products concerned with water usage in agriculture on the African continent. A great variety of different products, among them net- and gross-biomass-water-productivity, evaporation, transpiration, and interception as well as biomass production, are provided at a spatial resolution ranging between 250 meters up-to 30 meters for selected agricultural regions.\nThere are already some Python packages out there that allow users to programmatically access the WaPOR data portal such as hkvwaporpy or IHEWAwapor. However, I was not able to find similar functionality for R users. So I just went on and wrote an experimental package in R, which can be used to download raster data.\n\n\nWhat can it do?\nTo install and use the package, you should utilize remotes functionality.\n\nif(!\"wapoR\" %in% installed.packages()[,1]){\n  remotes::install_github(\"goergen95/wapoR\")\n}\nlibrary(wapoR)\n\nFrom there, it is quite straightforward to query available collections. Note that there are other collections available for which, in principal, it should be possible to download the data in the same way. But this package was primarily intended to interact with the WaPOR collections.\n\ncols = wapor_collections()\ncols[rev(seq(1,nrow(cols),)),c(1:2)]\n\n            code\n28          WPOP\n27         WATER\n26       WAPOR_2\n25           RVF\n24        RSCROP\n23      RICCAR_2\n22        RICCAR\n21          RDMS\n20          NMME\n19 NATURAL_EARTH\n18          NASA\n17          GLW4\n16          GLW3\n15           GLW\n14        GLEAM3\n13   GISMGR_TEST\n12       GAEZ_V4\n11     GAEZ_2015\n10      FROM_GLC\n9        FAOSTAT\n8           DLMF\n7           CRTB\n6      CROPWATCH\n5         CHIRPS\n4            C3S\n3        C2ATLAS\n2           ASIS\n1       AQUAMAPS\n                                                                       caption\n28                                                            WorldPop project\n27                                                                  Water Data\n26                           FAO Water Productivity Open-access portal (WaPOR)\n25                                                           Rift Valley Fever\n24                     Crop Pest and Disease Monitoring and Forecasting System\n23                              Regional Arab Climate Change Assessment Report\n22                              Regional Arab Climate Change Assessment Report\n21                                          Regional Drought Monitoring System\n20                                  North American Multi-Model Ensemble (NMME)\n19                                                               Natural Earth\n18                        National Aeronautics and Space Administration (NASA)\n17                                 Gridded Livestock of the World (GLW4, 2015)\n16                                 Gridded Livestock of the World (GLW3, 2010)\n15                                              Gridded Livestock of the World\n14                    Global Livestock Environmental Assessment Model (GLEAM3)\n13                      FAO GIS MANAGER (GISMGR) - Test and training workspace\n12                                         Global Agro-Ecological Zones (2021)\n11                                         Global Agro-Ecological Zones (2015)\n10 Finer Resolution Observation and Monitoring of Global Land Cover (FROM-GLC)\n9                                           FAO Corporate Statistical Database\n8                                     Desert Locust Monitoring and Forecasting\n7                                                  Climate Risk Toolbox (CRTB)\n6                                                                    CropWatch\n5           Climate Hazard group InfraRed Precipitation with Stations (CHIRPS)\n4                                            Copernicus Climate Change Service\n3                                                         Climate Change ATLAS\n2                                              Agriculture Stress Index System\n1                             Global spatial database on water and agriculture\n\n\nI reversed the order of the collections vector so that you can see that there are two available WaPOR collections representing version 1 and 2, respectively. I would advise using the updated version 2 if you do not have other reasons to use the first version.\nWe can query the available products within a collection by using wapor_products together with the collection we wish to query.\n\nprods = wapor_products(collection = \"WAPOR_2\")\n\nprint(paste0(\"In total there are \", length(prods), \" available products in the WAPOR_2 collection.\"))\n\n[1] \"In total there are 285 available products in the WAPOR_2 collection.\"\n\nstr(prods[1])\n\nList of 1\n $ L1_GBWP_A:List of 2\n  ..$ product:'data.frame': 1 obs. of  3 variables:\n  .. ..$ code       : chr \"L1_GBWP_A\"\n  .. ..$ caption    : chr \"Gross Biomass Water Productivity\"\n  .. ..$ description: chr \"The annual Gross Biomass Water Productivity expresses the quantity of output (total biomass production) in rela\"| __truncated__\n  ..$ meta   :'data.frame': 1 obs. of  12 variables:\n  .. ..$ format                : chr \"Raster Dataset\"\n  .. ..$ unit                  : chr \"kg/m³ is the ratio of kg of dry matter per cubic meter of water transpired by vegetation in one hectare\"\n  .. ..$ dataType              : chr \"Int32 (32bit Integer)\"\n  .. ..$ conversionFactor      : chr \"the pixel value in the downloaded data must be multiplied by 0.001\"\n  .. ..$ noDataValue           : int -9999\n  .. ..$ spatialResolution     : chr \"250m (0.00223 degree)\"\n  .. ..$ spatialExtent         : chr \"Africa and Near East\"\n  .. ..$ spatialReferenceSystem: chr \"EPSG:4326 - WGS84 - Geographic Coordinate System (lat/long)\"\n  .. ..$ temporalResolution    : chr \"from January 2009 to present\"\n  .. ..$ temporalExtent        : chr \"Annual\"\n  .. ..$ nearRealTime          : chr \"New dekadal data layers are released approximately 5 days after the end of a dekad. A higher quality version of\"| __truncated__\n  .. ..$ methodology           : chr \"The calculation of gross biomass water productivity (GBWP) is as follows: GBWP = TBP/ETIa Where TBP is annual T\"| __truncated__\n\nnames(prods)[1:10]\n\n [1] \"L1_GBWP_A\" \"L1_NBWP_A\" \"L1_AETI_A\" \"L1_AETI_M\" \"L1_AETI_D\" \"L1_T_A\"   \n [7] \"L1_E_A\"    \"L1_I_A\"    \"L1_T_D\"    \"L1_E_D\"   \n\n\nThe total number of products is relatively high. The product names consist first of the level a respective product belongs to. Level 1 means this product belongs to the continental products covering the African continent at a spatial resolution of about 250 meters. Level 2 products show a resolution of 100 meters. However, they are only available for selected countries. Finally, level 3 data is available for only a few specific agricultural regions, but the spatial resolution is about 30 meters.\nThe second component in the product name specifies the variable. For example, GBWP stands for Gross Biomass Water Productivity or AETI for Actual Evapotranspiraton and Interception. You can check out the WaPOR catalog to see all available products, or you search through the product list as some metadata is also included in the above object.\nThe last component of a product name specifies its temporal resolution, where A stands for annual, M for monthly, D for decadal, and S for a seasonal temporal resolution.\nLet’s assume we decided to download some level 3 data for the Office du Niger agricultural region. First, let’s take a look at the available products:\n\nnames(prods)[grep(\"L3_ODN\", names(prods))]\n\n [1] \"L3_ODN_GBWP_S\"       \"L3_ODN_NBWP_S\"       \"L3_ODN_AETI_A\"      \n [4] \"L3_ODN_AETI_M\"       \"L3_ODN_AETI_D\"       \"L3_ODN_T_A\"         \n [7] \"L3_ODN_E_A\"          \"L3_ODN_I_A\"          \"L3_ODN_T_D\"         \n[10] \"L3_ODN_E_D\"          \"L3_ODN_I_D\"          \"L3_ODN_NPP_M\"       \n[13] \"L3_ODN_NPP_D\"        \"L3_ODN_TBP_S\"        \"L3_ODN_LCC_D\"       \n[16] \"L3_ODN_PHE_S\"        \"L3_ODN_QUAL_LCC_S\"   \"L3_ODN_QUAL_NDVI_D\" \n[19] \"L3_ODN_QUAL_NDVI_LT\"\n\n\nFor the sake of a quick example, let’s say we are interested in the actual evapotranspiration and interception for the year 2018. We can query some additional metadata about this product with the following command:\n\nmeta = wapor_metadata(collection = \"WAPOR_2\", product = \"L3_ODN_AETI_A\")\nstr(meta)\n\nList of 3\n $ info      :'data.frame': 1 obs. of  5 variables:\n  ..$ code      : chr \"WATER_MM\"\n  ..$ caption   : chr \"Amount of Water\"\n  ..$ unit      : chr \"mm\"\n  ..$ scale     : int 3\n  ..$ multiplier: num 0.1\n $ dimensions:'data.frame': 1 obs. of  3 variables:\n  ..$ code   : chr \"YEAR\"\n  ..$ caption: chr \"Year\"\n  ..$ type   : chr \"TIME\"\n $ meta      :'data.frame': 1 obs. of  12 variables:\n  ..$ format                : chr \"Raster Dataset\"\n  ..$ unit                  : chr \"mm\"\n  ..$ dataType              : chr \"Int32 (32bit Integer)\"\n  ..$ conversionFactor      : chr \"the pixel value in the downloaded data must be multiplied by 0.1\"\n  ..$ noDataValue           : int -9999\n  ..$ spatialResolution     : chr \"30m\"\n  ..$ spatialExtent         : chr \"Office du Niger, Mali\"\n  ..$ spatialReferenceSystem: chr \"EPSG:32630 - WGS 84 / UTM zone 30N\"\n  ..$ temporalResolution    : chr \"from January 2009 to present\"\n  ..$ temporalExtent        : chr \"Annual\"\n  ..$ nearRealTime          : chr \"New dekadal data layers are released approximately 5 days after the end of a dekad. A higher quality version of\"| __truncated__\n  ..$ methodology           : chr \"See ETIa by dekad for further information. The annual total is obtained by taking the ETIa in mm/day, multiplyi\"| __truncated__\n\n\nFrom the above, we already get a lot of useful information. For example, we see that the product is available between 2009 and the current year and is provided in a projected coordinate reference system. We also can see that the unit of the pixel values is in millimeters bit that the pixel value shall be multiplied with a scale factor of 0.1. This is essential information, and it should be checked for all WaPOR products since most of them were rescaled to reduce file size.\nMaybe the most important aspect of the above output for the next step, is the dimensions dataframe. Here we can see that the selected product only shows one dimension called “YEAR” which type is time. Other products might have further dimensions, such as “SEASON” which needs different specification in the download call.\n\nwapor_queryRaster(collection = \"WAPOR_2\",\n                  product = \"L3_ODN_AETI_A\",\n                  begin = \"2018-01-01\", # begin date is inclusive\n                  end = \"2019-01-01\", # end date is exclusive\n                  outdir = \".\",\n                  APIkey = Sys.getenv(\"WAPOR-KEY\")) \n\nLet’s take a glimpse at the data we just downloaded.\n\nlibrary(raster)\n\nLoading required package: sp\n\nfile = list.files(\".\", \"L3_ODN\", full.names = T)\nr = raster(file) * 0.1\nplot(r)\n\n\n\n\nWe very quickly downloaded some important data for an assessment of agricultural practices in ODN. This package’s download functionality can be used to download specific regions from Level 2 or 3 datasets by providing an sf object of an area of interest. Additionally, complete-time series can be downloaded by adapting the start and end date. Check out the README of the package for another example to download some data and leave an issue if you face any problems using this package."
  },
  {
    "objectID": "contents/blog/2022-08-30-quarto/index.html",
    "href": "contents/blog/2022-08-30-quarto/index.html",
    "title": "Migrating to Quarto",
    "section": "",
    "text": "I recently migrated this very website that you are seeing to the Quarto engine. With this post, I want to share some of my experiences of the migration process as well as some general thoughts on website generation with Quarto. To start with, it has really been a fun experience to set up this website using Quarto. Even though the tool is quite new and has only been publicly announced at rstudio::conf(2022) some time ago, it really is thoughtfully engineered, feature rich and easy to use.\nTo set up my motivation to migrate to Quarto, let me give you some insights of how I created content for my website before. I have been using the quite familiar Indigo theme base on Jekyll for some time. I liked it very much for its minimalist look and feel. However, creating content and get it “out there” was not quite a minimalist experience for me. Partly, that was due to my desire to feature both, content on R and Python, and very likely also due to my inexperience with websites more general. In order for you to get the picture, let me first explain how I created content using Jekyll and then what changed for me now that I am using Quarto."
  },
  {
    "objectID": "contents/blog/2022-08-30-quarto/index.html#the-tidious-way-to-create-content",
    "href": "contents/blog/2022-08-30-quarto/index.html#the-tidious-way-to-create-content",
    "title": "Migrating to Quarto",
    "section": "The tidious way to create content…",
    "text": "The tidious way to create content…\nIn my previous set up using Jekyll I had a _drafts directory. There I would write some R Markdown files drafting my posts. Thanks to the awesome {reticulate} package I could already use Python in these R Markdown files using a virtual environment. I would add the following yaml header to all drafts:\n---\noutput:\n html_document:\n   keep_md: true\n---\nThat would tell the knitr engine to create both, a .html and a .md file. The files would all be rendered within the _drafts directory. I then would have to manually create another <post>-header.md file including the header information for the post I was currently working on, but nothing else:\n---\ntitle: \"Some title\"\ndescription: \"Some more description\"\nlayout: post\ndate: 2020-05-10 10:00\ntag: \n - msg\n - gtiff\n - python\nheaderImage: false\nhidden: false # don't count this post in blog pagination\ncategory: blog or project?\nauthor: darius\n---\nThe template repository that I was using thankfully included a bash file that would then allow me to select the name of a draft post. This script would take the .md that was outputted by {knitr}, add the yaml header that I created and then copy this new file over to the _posts directory from where the website would be available once checked out to GitHub. Referring to assets that should show up in a post was tedious and more than once error-prone. As you can imagine, I was very much keen to use an alternative, simpler way to handle my website. Entrance Quarto!"
  },
  {
    "objectID": "contents/blog/2022-08-30-quarto/index.html#designing-a-website-with-quarto",
    "href": "contents/blog/2022-08-30-quarto/index.html#designing-a-website-with-quarto",
    "title": "Migrating to Quarto",
    "section": "Designing a website with Quarto",
    "text": "Designing a website with Quarto\nThe general directory structure of my Quarto website (restricted here to the most important parts) is actually quite simple:\n.\n├── contents\n      ├── blog\n      └──  projects\n├── _quarto.yaml\n├── index.qmd\n├── renv\n└── pyenv\nI structure my contents into a blog and some information snippets about projects I have been working on. Each of those contain single directories for each post. Within a post directory, I include an index.qmd file and all assets that are to be referenced by this post. Note, that within the .qmd I can use R or Python at ease. When rendering, Quarto will automatically activate the virtual environment pyenv for Python code or the renv for R files (more on that later). The main landing page is index.qmd thus it is found in the top-level of the repository. Website-wide configurations, like the navbar, are included in _quarto.yaml. It is there that I specified to freeze the rendering of posts. That way quarto will only try to re-render posts if it finds changes in the source code. If that is not the case, the previous outputs will simply be copied to the _site directory where the rendered website is found. This is a very useful behavior, especially over long-time frames when package version changes might make it difficult to re-render older posts.\nWithin each .qmd, I now include the yaml header that I had to create manually before. So instead of actually 4 different files that I created to publish a single post that was then copied to the right location, I now use a single-source-of-truth, if you will, that is automatically rendered if needed once I issue quarto render to the command line. This really is a significant improvement of my personal workflow to put out new content! The source repository is also much cleaner and easier to argue about and troubleshoot if something goes wrong."
  },
  {
    "objectID": "contents/blog/2022-08-30-quarto/index.html#seamless-integration-of-python-and-r-in-the-same-project",
    "href": "contents/blog/2022-08-30-quarto/index.html#seamless-integration-of-python-and-r-in-the-same-project",
    "title": "Migrating to Quarto",
    "section": "Seamless integration of Python and R in the same project",
    "text": "Seamless integration of Python and R in the same project\nI am amazed by the seamless integration of both Python and R into a single project. For the R side, I am using {renv} for the housekeeping of packages. For Python I simply created a virtual environment via python3 -m venv pyenv to create a virtual environment within the project directory. Quarto will automatically activate this environment when executing Python code. I (or others) can even set up the project easily on a different machine by updating the requirements.txt every once in a while by using python3 -m pip freeze > requirements.txt. I mentioned earlier, that the freeze option is a useful feature over long time periods. It is also a very useful when collaboration with others on a website potentially using R and Python. Every contributor might only make small changes to single parts of the website. Using the freeze options ensures that the website as a whole stays intact, even for different language or version requirements between the posts."
  },
  {
    "objectID": "contents/blog/2022-08-30-quarto/index.html#publishing-with-quarto",
    "href": "contents/blog/2022-08-30-quarto/index.html#publishing-with-quarto",
    "title": "Migrating to Quarto",
    "section": "Publishing with Quarto",
    "text": "Publishing with Quarto\nMy first idea to publish the website was using GitHub Actions to automatically build the site once changes to the main branch are checked out. While in principal this is possible, this approach has two major drawbacks. First, when version requirements are different between some posts this approach will fail when trying to set up the environment on GitHub Actions. This could be mitigated by ensuring that all posts are kept up-to-date with package developments, but maintaining this website should not be a full-time job. Second, setting up both the R and Python environment on GitHub actions takes quite some time and I would not like to wait about 30 minutes between checking out some changes and the deployment of the website. I briefly was thinking about changing the OS of the workflow to MacOS or Windows, hoping that using binaries would speed up the process. However, Quarto comes with a very nice feature which allows me to directly publish to the gh-pages branch from my local machine. Once I am satisfied with my changes I first checkout to the main branch. Then I issue quarto publish gh-pages and because of the freeze option being enabled, this quickly pushes the website to GitHub where it is then deployed. This actually saves some significant computation times on GitHub servers reducing the carbon footprint of this website."
  },
  {
    "objectID": "contents/blog/2022-08-30-quarto/index.html#round-up",
    "href": "contents/blog/2022-08-30-quarto/index.html#round-up",
    "title": "Migrating to Quarto",
    "section": "Round up",
    "text": "Round up\nSetting up a website using Quarto and customizing it has been a very fun experience. The design of the tool is very user-friendly and it helped me to improve the way I publish on this website. The freeze option is potentially saving an incredible amount of headaches for both single contributors who have to maintain state over longer periods of time and collaborators potentially using different programming languages. Posit (formerly known as RStudio) truly advances the way how we can work in multi-lingual projects and reduces the overhead associated with sharing content. I have been a great fan of R Markdown over the last couple of years. R Markdown has grown for over 10 years now. Quarto is meant to be its successor and I can’t wait to see what else will be possible with it over the next 10 years!"
  },
  {
    "objectID": "contents/projects.html",
    "href": "contents/projects.html",
    "title": "Darius A. Görgen",
    "section": "",
    "text": "Tree species classification based on UAV orthoimages\n\n\n\n\n\n\n\nUAV\n\n\nRandomForest\n\n\nR\n\n\n\n\nExploration of tree species classification accuracy.\n\n\n\n\n\n\nApr 17, 2020\n\n\nDarius A. Görgen\n\n\n\n\n\n\n  \n\n\n\n\nAerosol-Cloud Interactions and Precipitation in the Aral Sea basin\n\n\n\n\n\n\n\nMODIS\n\n\nClouds\n\n\nPrecipitaion\n\n\nR\n\n\n\n\nAnalysis of cloud-aersol interactions in the Aral Sea basin.\n\n\n\n\n\n\nJan 20, 2020\n\n\nDarius A. Görgen\n\n\n\n\n\n\n  \n\n\n\n\nMachine Learning to classify microplastic particles\n\n\n\n\n\n\n\nMachine Learning\n\n\nMicroplastics\n\n\nCNN\n\n\nFTIR-Spectrometry\n\n\nR\n\n\n\n\nClassification of Fourier-Transform Spectra to classify microplastics.\n\n\n\n\n\n\nOct 22, 2019\n\n\nDarius A. Görgen\n\n\n\n\n\n\n  \n\n\n\n\nLow-cost SensorBoxes for automated environmental monitoring\n\n\n\n\n\n\n\nEnvironment\n\n\nMonitoring\n\n\nRaspberry Pi\n\n\n\n\nLow-cost environmental sensor unit based on a Raspberry Pi 3.\n\n\n\n\n\n\nJul 20, 2019\n\n\nDarius A. Görgen\n\n\n\n\n\n\n  \n\n\n\n\nForest stand analysis based on remote sensing\n\n\n\n\n\n\n\nMachine Learning\n\n\nLIDAR\n\n\nR\n\n\n\n\nA project using RGB images and LIDAR data to classify tree species and analyse forest stand structures.\n\n\n\n\n\n\nMay 31, 2019\n\n\nDarius A. Görgen\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "contents/projects/2019-05-31-mof/index.html",
    "href": "contents/projects/2019-05-31-mof/index.html",
    "title": "Forest stand analysis based on remote sensing",
    "section": "",
    "text": "Tree species biodiversity map.\n\nWhat is it about?\nForests represent the most diverse habitat for different species around the globe. Their monitoring is one of the most crucial tasks for biodiversity management. Traditional means of monitoring forests are cost and labor-intensive, which leads to low revisit frequencies and small monitored areas. Additionally, the results of data acquisitions by human agents make the results hardly reproducible. To overcome these limitations, a LOEWE research project called Natur4.0 has been initiated between several German research institutes. In this project, I participated in a student’s seminar and analyzed tree species and forest structures employing remote sensing techniques.\n\n\nWhat it can do!\nIn this project, I used RGB orthoimages and a point cloud derived from Light Detection and Ranging (LiDAR) data to train a segmentation algorithm to distinguish between individual trees based on a Canopy Height Model. This technique is based on a watershed algorithm that “grows” continues segments around a tree’s central position to delineate the total tree crown.\n\n\n\nAnimation of the tree segmentation\n\n\nOnce we successfully generated the tree objects, we used an object-based classification of the tree species. As predictors, we used several artificially created indices and filters from RGB images. Finally, we trained a Random Forest model to predict the tree species. Using the point cloud, structural forest parameters, such as vegetation density, can be aggregated on the level of individual trees and then analyzed. We see this result in the picture above where I calculated the Shannon-Index based on the number of different tree species found in a circular 10 meters environment. Green colors show lower numbers of tree species, while red colors indicate a relative species richness.\n\nYou can check out the results on a comprehensive website! You are also invited to read through the code for the analysis."
  },
  {
    "objectID": "contents/projects/2019-07-01-sensorboxes/index.html",
    "href": "contents/projects/2019-07-01-sensorboxes/index.html",
    "title": "Low-cost SensorBoxes for automated environmental monitoring",
    "section": "",
    "text": "What is it about?\nThis project website documents a master’s seminar work at the University of Marburg on a fully automated, low-cost, and self-build environmental sensor unit used in the LOEWE research project Natur4.0. Its core part is a Raspberry Pi 3 (Model B) equipped with sensors to measure incoming radiation, temperature, and humidity, as well as a camera and a microphone to take records of animals in forest environments.\n\nCore Components\n\nRaspberry Pi 3 (B)\nRaspberry Pi Camera Module v2.1\nSF-555 Microphone (Foxnovo)\nDHT22-AM2302 Temperature and Humidity Sensor\nTSL2591 High Dynamic Range Digital Light Sensor\nKY-024 Hall Sensors\nDS3231 Real-Time Clock\nPowerBank and Wireless Charging Pads\n\n\n\n\nWhat it can do!\nThe SensorBox is a fully automated sensing unit that can be used in conjunction with a cable car mounted on specific trees to monitor environmental variables on a vertical gradient within forest structures. There is a need to regularly charge the batteries on the ground station and in case of the presence of a WLAN infrastructure, the SensorBox can send the collected data to another station. It can help to collect data on valuable parameters which can be used together with remotely sensed imagery, e.g., by UAVs, to predict the spatial distribution of these parameters below the canopy cover.\n\nCheck it out here or follow the link to GitHub to explore the source code of the website. Note that the content shown here comes from a student project and only covers the status during our two-week seminar. Check out the official documentation for more recent information."
  },
  {
    "objectID": "contents/projects/2019-10-15-polymeRID/index.html",
    "href": "contents/projects/2019-10-15-polymeRID/index.html",
    "title": "Machine Learning to classify microplastic particles",
    "section": "",
    "text": "What is it about?\nMicroplastic particles in the environment are an ever-growing concern in public and science. It is suspected not only to pose threats to wildlife but also concerns for human health have been raised. Scientists and especially decision-makers depend on the availability of precise and timely information on the where and when’s of microplastic in the environment. However, the process of spectral identification of particles remains a cost-intensive process requiring humans to invest time and effort into the correct identification of spectra. Modern machine learning techniques have the potential to decrease the necessity of human intervention in the classification process and thus significantly speed up the complete process from taking environmental samples to communicate the results to the public.\n\n\nWhat it can do!\nBased on a freely available reference data base published by Primpke and colleagues I developed a decision fusion algorithm based on two Random Forest models and two Convolutional Neural Networks which together can robustly identify polymers based on FTIR-spectral data. The complete workflow is reproducible and adjustable and consists of the following stages: preparation of a reference data base, exploration of machine learning models and pre-processing techniques, calibration of the final decision fusion algorithm, and classification of real-world spectral samples. The algorithm was written in R and the code is available on a GitHub repository. Additionally, there is a workflowr website communicating the structure of the algorithm and the results based on the mentioned OpenSource data base.\n\nHere you can explore the spectral reference data base based on selected polymers and other materials commonly found in environmental samples. Simply choose a class you wish to display and explore its spectral characteristics."
  },
  {
    "objectID": "contents/projects/2020-03-20-aciASB/index.html",
    "href": "contents/projects/2020-03-20-aciASB/index.html",
    "title": "Aerosol-Cloud Interactions and Precipitation in the Aral Sea basin",
    "section": "",
    "text": "What it can do!\nThe mechanisms which govern the cloud-aerosol interactions mainly depend on the type of aerosol, whether it acts hydrophilic or hydrophobic, and its size. Some mechanisms are expected to suppress precipitation, while others are suspected of leading to more severe and intense precipitation events. To investigate the relationship between aerosols and precipitation, I made use of the MODIS cloud and aerosol products as well as the CHIRPS data set.\nThe former uses satellite and ground observations to retrieve rainfall rates at a monthly resolution. The cloud and aerosol parameters were aggregated to the same spatial and temporal resolution to allow a correlation analysis.\n\n\n\n\nCorrelation AOD and P\n\n\nDuring spring (a) and winter (d), a negative relationship between the Aerosol Optical Depth (AOD) and precipitation rates (P) govern the study area. During the other two seasons, we also see pixels with positive correlations. However, these are not significant (pixels with a significant correlation are marked by crosses). The analysis was done in R and included data from 2003 to 2018 because since then, both MODIS satellites delivered observations for the study area. The source code of the complete project can be found in a GitHub repository, along with a more comprehensive discussion of the results."
  },
  {
    "objectID": "contents/projects/2020-04-17-pheno/index.html",
    "href": "contents/projects/2020-04-17-pheno/index.html",
    "title": "Tree species classification based on UAV orthoimages",
    "section": "",
    "text": "What is it about?\nSupervised machine learning algorithms can help to extract useful information from high-dimensional datasets to benefit environmental conversation efforts. The correct identification of tree species based on imagery collected by low-cost UAVs and conventional cameras is undoubtedly a promising advancement in technology which might reduce the cost of local forest monitoring. Because the broader use of this technology only occurred very recently, structural investigations into the benefits and the limitations of this approach are limited. As a student’s team, we set out to investigate the relationship between the classification accuracy and different spatial resolutions of the imagery. Additionally, we were interested in the question of whether or not predictor variables calculated based on multi-temporal observations throughout the growing season enhances the classification.\n\n\nWhat can it do?\nWe established an empirical experiment to investigate the influence of spatial resolution and mono- vs. multi-temporal predictor variables using the Random Forest algorithm. We used 5-fold cross-validation combined with the Leave-Location-Out approach (LLOCV) to train a total number of 9 models. These included each combination of three different spatial resolutions (10, 15, and 25 cm) and three different combinations of the predictor variables (mono-temporal, seasonal, and both predictor sets.)\n\n\n\nAccuracies of a 5-fold cross-validation\n\n\nWe learned that a medium resolutions seems beneficial and that seasonal parameters are able to increase the classification accuracy at about 1-2 %. There were also indications, that object-based classification has the potential to significantly increase the overall accuracy.\n\nInterested in the results? Check out our written report or browse through our R code workflow."
  },
  {
    "objectID": "impressum.html",
    "href": "impressum.html",
    "title": "Impressum",
    "section": "",
    "text": "Darius A. Görgen Wehrdaer Weg 32 A 35037 Marburg\nKontakt: Telefon: 0157-51157081 E-Mail: Darius2402@web.de\nVerantwortlich für den Inhalt nach § 55 Abs. 2 RStV: Darius A. Görgen Wehrdaer Weg 32 A 35037 Marburg"
  },
  {
    "objectID": "impressum.html#haftungsausschluss",
    "href": "impressum.html#haftungsausschluss",
    "title": "Impressum",
    "section": "Haftungsausschluss:",
    "text": "Haftungsausschluss:\n\nHaftung für Inhalte\nDie Inhalte unserer Seiten wurden mit größter Sorgfalt erstellt. Für die Richtigkeit, Vollständigkeit und Aktualität der Inhalte können wir jedoch keine Gewähr übernehmen. Als Diensteanbieter sind wir gemäß § 7 Abs.1 TMG für eigene Inhalte auf diesen Seiten nach den allgemeinen Gesetzen verantwortlich. Nach §§ 8 bis 10 TMG sind wir als Diensteanbieter jedoch nicht verpflichtet, übermittelte oder gespeicherte fremde Informationen zu überwachen oder nach Umständen zu forschen, die auf eine rechtswidrige Tätigkeit hinweisen. Verpflichtungen zur Entfernung oder Sperrung der Nutzung von Informationen nach den allgemeinen Gesetzen bleiben hiervon unberührt. Eine diesbezügliche Haftung ist jedoch erst ab dem Zeitpunkt der Kenntnis einer konkreten Rechtsverletzung möglich. Bei Bekanntwerden von entsprechenden Rechtsverletzungen werden wir diese Inhalte umgehend entfernen.\n\n\nHaftung für Links\nUnser Angebot enthält Links zu externen Webseiten Dritter, auf deren Inhalte wir keinen Einfluss haben. Deshalb können wir für diese fremden Inhalte auch keine Gewähr übernehmen. Für die Inhalte der verlinkten Seiten ist stets der jeweilige Anbieter oder Betreiber der Seiten verantwortlich. Die verlinkten Seiten wurden zum Zeitpunkt der Verlinkung auf mögliche Rechtsverstöße überprüft. Rechtswidrige Inhalte waren zum Zeitpunkt der Verlinkung nicht erkennbar. Eine permanente inhaltliche Kontrolle der verlinkten Seiten ist jedoch ohne konkrete Anhaltspunkte einer Rechtsverletzung nicht zumutbar. Bei Bekanntwerden von Rechtsverletzungen werden wir derartige Links umgehend entfernen.\n\n\nUrheberrecht\nDie durch die Seitenbetreiber erstellten Inhalte und Werke auf diesen Seiten unterliegen dem deutschen Urheberrecht. Die Vervielfältigung, Bearbeitung, Verbreitung und jede Art der Verwertung außerhalb der Grenzen des Urheberrechtes bedürfen der schriftlichen Zustimmung des jeweiligen Autors bzw. Erstellers. Downloads und Kopien dieser Seite sind nur für den privaten, nicht kommerziellen Gebrauch gestattet. Soweit die Inhalte auf dieser Seite nicht vom Betreiber erstellt wurden, werden die Urheberrechte Dritter beachtet. Insbesondere werden Inhalte Dritter als solche gekennzeichnet. Sollten Sie trotzdem auf eine Urheberrechtsverletzung aufmerksam werden, bitten wir um einen entsprechenden Hinweis. Bei Bekanntwerden von Rechtsverletzungen werden wir derartige Inhalte umgehend entfernen.\nImpressum vom Impressum Generator der Kanzlei Hasselbach, Frankfurt"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Darius A. Görgen",
    "section": "",
    "text": "I am anworking on a range of topics concerned with environmental change and the associated coping strategies of our societies. I have been studying Geography and Political Sciences of the MENA region at the University of Marburg, focusing on Remote Sensing, Climatology, and Hydrogeography. In my projects I apply my Data Science skills to help teams to generate information for better decision making. Currently, I am working as a research associate at the Federal Institute for Geosciences and Resources. I also offer freelance consulting services using R and Python for reproducible analysis of spatial data.\n\n\n\nR\nR Markdown\nQuarto\nPython\nShiny\nDocker\nData Analysis\nMachine Learning\nGeographic Information System\nPostgreSQL\nBash\n\n\n\n\nMAPME Initiative: Maps for Planning, Monitoring, and Evaluation in the Development Cooperation Sector, funded by KfW, Contributer to several software packages www.mapme-initiative.org.\n{mapme.biodiversity}: Maintainer of an R package for the collection of variables for global biodiversity portfolios.\nPredicting violent conflict in Africa - Leveraging open geodata and deep learning for spatiotemporal event detection, M.Sc. thesis at the University of Marburg, submitted to the Chair of Climatology and Remote Sensing and the Chair of Environmental Informatics. (https://goergen95.github.io/thesis-predicting-conflict)"
  }
]
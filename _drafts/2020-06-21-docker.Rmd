---
output:
 html_document:
   keep_md: true
---

In this tutorial I am going to show how you can use the docker engine to set up 
two containers, one hosting the [rocker/geospatial](https://hub.docker.com/r/rocker/geospatial) 
image and the other one the [postgis/postgis](https://hub.docker.com/r/postgis/postgis)
image. We will use docker-compose to start up both containers
so we can work with the r-spatial packages and postgis functionality simultaneously
in a stable environment.
I assume here that the necessary dependencies such as the [docker](https://docs.docker.com/get-docker/) and 
[docker-compose](https://docs.docker.com/compose/install/) engines are already met.

---

What we need is a simple .yaml file which defines the parameters of our application.
In the example I am going to demonstrate, the application consists of two services,
namely the postigs server and the R-Studio instance. We start with the postgres 
service by defining which images we are using. This image is found at [dockerhub](https://hub.docker.com/)
so in case the images are not found on the local machine they are pulled from there
once we run our application. We also make the naming of the container explicit by
using the _container_name_ option and set the restart behavior to _always_ in cases
the container breaks down. We also make sure to that the container will set up
a new user besides the default _postgres_ user by specifying a username and a password.
The last part specifies on the left hand the local path on the host machine where 
the database files are going to be written two, while the right hand part
links to the data directory within the postgis container.

```{yaml, eval=F}
services:
  postgres:
  image: postgis/postgis
  container_name: postgres
  restart: always
  environment:
    POSTGRES_USER: postgis
    POSTGRES_PASSWORD: postgis
  volumes:
    - ~/pgdata:/var/lib/postgresql/data

rstudio:
  image: rocker/geospatial
  container_name: r-studio
  restart: always
  environment:
    - USER=rstudio
    - PASSWORD=supersecret
    - ROOT=TRUE
  ports:
    - 8787:8787
  links: 
    - postgres
```

For the R-Studio service, we are using the geospatial image which comes in very
handy in cases you want to do geospatial analysis since a large number of important
packages already come preinstalled. We also make the naming of the container explicit
and set the restart behaviour to _always_. Since the rocker images are quite restrictive
when it comes to user rights management, in addition to specifying a user and a 
password we also want to enable root access in cases we need to install additional
software. This way our user is added to the sudoer list. However, this option
should be treated carefully. We also map a port on the host machine to the exposed
port of the container. This way we are able to reach the interface by simply 
accessing *http://localhost:8787/* in the browser of our choice. Finally, we declare
that the _posgres_ service is linked to the _rstudio_ service. This means that inside
the rstudio container an entry to _/etc/hosts_ called postgres is added linking
to the IP-address this container is found.

When we write the above configuration to a file called _docker-compose.yaml_ starting
the application is as simple as running the following command in a shell in the
same directory the file is found:

```{bash, eval=F}
docker-compose up
```

Once the services are up and running visit *http://localhost:8787/* and enter 
the username and password for the rstudio service. From here we can use the _RPostgreSQL_
and _RPostgres_ library to connect to our data base. In the simple example below,
we establish a connection and write a sf-Object shipped with the _sf_ package to the 
database.

```{r eval=FALSE}
library(RPostgreSQL)
library(RPostgres)
library(sf)

# specify connection inputs
db <- 'postgis'  # provide the name of your db
host_db = "postgres" # provide the name of the service
db_port = '5432'  # specify the port the service is available at
db_user = "postgis"  # specify username
db_password = "postgis" # specify password
# establish the connection
con <- dbConnect(Postgres(), 
                 dbname=db, 
                 host=host_db, 
                 port=db_port, 
                 user=db_user, 
                 password=db_password)

# read sample shapefile
shape = st_read(system.file("gpkg/nc.gpkg", package = "sf"))
# write to database in a new table specified by the layer argument
table_name = "test"
st_write(obj = shape, dsn = con, layer = table_name)

```

Since we mapped the data directory to a local directory on our host machine
the data we write into the database is persistent even when the application 
is no longer running. So to query, our database now or even after restarting the 
application we can use more sf functionality to interact with it.
Here is a simple example comparing dplyr syntax with the result we obtain from
querying the database to unify all polygons which have the same entry in the 'SID74' 
variable.

```{r, eval=F}
library(dplyr)

# dplyr way of 
shape_dplyr = shape %>%
  group_by(SID74) %>%
  summarise(AREA = sum(AREA, na.rm = TRUE))

shape_postigs = st_read(con, query = sprintf(paste0('SELECT "SID74", SUM("AREA") as "AREA", ST_UNION(geom) as geom \n',
                                    'FROM \"%s\" \n',
                                    'GROUP BY "SID74";')), table_name)
```



